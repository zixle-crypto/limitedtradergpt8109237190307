only cache what users request, then keep those items “hot” with automatic refresh. This gives you the instant Rolimon’s feel without getting rate-limited.

Below is a drop-in script patch you can paste into your existing FastAPI bridge. It adds:

✅ Cache-first /market/item/analyze (instant responses)
✅ Background refresher (updates requested items every ~10s)
✅ SQLite persistence (survives restarts)
✅ “stale-while-revalidate” (returns cached even if refresh fails)

1) Add these imports at the top
import asyncio
import json
import sqlite3
from collections import defaultdict
from typing import Tuple

2) Add this cache config + DB helpers (near your Config section)
DB_PATH = "market_cache.sqlite3"

# Cache behavior
CACHE_TTL_SECONDS = 120          # consider cached "fresh" for 2 minutes
HOT_REFRESH_SECONDS = 10         # refresh hot items every 10 seconds
MAX_REFRESH_PER_CYCLE = 8        # safety cap (prevents spam)
SLEEP_TICK_SECONDS = 1           # background loop tick

# In-memory cache:
# key -> {"updated_at": float, "payload": dict}
CACHE: dict[str, dict] = {}

# Hot items tracking (requested recently)
# key -> last requested time, request count
HOT_LAST_SEEN: dict[str, float] = {}
HOT_COUNT: defaultdict[str, int] = defaultdict(int)

# Simple global limiter (very basic)
LAST_UPSTREAM_CALL_AT = 0.0
MIN_SECONDS_BETWEEN_UPSTREAM_CALLS = 0.2  # 5 req/sec max overall

DB init + get/put
def db_init():
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("""
      CREATE TABLE IF NOT EXISTS item_cache (
        cache_key TEXT PRIMARY KEY,
        updated_at REAL NOT NULL,
        payload TEXT NOT NULL
      )
    """)
    con.commit()
    con.close()

def db_get(cache_key: str):
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("SELECT updated_at, payload FROM item_cache WHERE cache_key=?", (cache_key,))
    row = cur.fetchone()
    con.close()
    if not row:
        return None
    updated_at, payload = row
    return {"updated_at": float(updated_at), "payload": json.loads(payload)}

def db_put(cache_key: str, payload: dict):
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute(
        "INSERT OR REPLACE INTO item_cache(cache_key, updated_at, payload) VALUES(?, ?, ?)",
        (cache_key, time.time(), json.dumps(payload)),
    )
    con.commit()
    con.close()

Cache helpers
def cache_key_for_asset(asset_id: int) -> str:
    return f"item:{asset_id}"

def cache_get(cache_key: str):
    # memory first
    if cache_key in CACHE:
        return CACHE[cache_key]
    # db fallback
    row = db_get(cache_key)
    if row:
        CACHE[cache_key] = row
        return row
    return None

def cache_put(cache_key: str, payload: dict):
    row = {"updated_at": time.time(), "payload": payload}
    CACHE[cache_key] = row
    db_put(cache_key, payload)

def is_fresh(updated_at: float) -> bool:
    return (time.time() - updated_at) < CACHE_TTL_SECONDS

def mark_hot(cache_key: str):
    HOT_LAST_SEEN[cache_key] = time.time()
    HOT_COUNT[cache_key] += 1

3) Add a tiny upstream throttle (optional but recommended)

Add this helper:

async def _global_throttle():
    global LAST_UPSTREAM_CALL_AT
    now = time.time()
    wait = (LAST_UPSTREAM_CALL_AT + MIN_SECONDS_BETWEEN_UPSTREAM_CALLS) - now
    if wait > 0:
        await asyncio.sleep(wait)
    LAST_UPSTREAM_CALL_AT = time.time()


Then in your _http_get_json and _http_post_json, you can leave them sync as-is — we’ll throttle in the background worker (so user requests still feel fast).

4) Build a “compact payload” builder (this is what gets cached)

This assumes you already have these functions from your Roblox-only version:

get_catalog_details(asset_id)

dual-path resale/history functions (optional but recommended)

get_resellers(asset_id)

summarize_resellers(payload)

compute_market_stats(resale_data, resale_history)

Paste this:

def build_compact_item_payload(asset_id: int, history_points: int = 120) -> dict:
    roblox = get_catalog_details(asset_id)
    collectible_item_id = _extract_collectible_item_id(roblox)

    market_notes = []
    resale_data = {}
    resale_history = {"data": []}

    # Resellers snapshot (best for traders)
    listings = {"count": 0, "lowest_price": None, "top_5": []}
    try:
        resellers_raw = get_resellers(asset_id, limit=30)
        listings = summarize_resellers(resellers_raw)
    except HTTPException as e:
        market_notes.append(f"Resellers unavailable (status {e.status_code}).")

    # Resale data/history (may 404; don't fail)
    try:
        resale_data = get_resale_data_dual(asset_id, collectible_item_id)
    except HTTPException as e:
        market_notes.append(f"Resale-data unavailable (status {e.status_code}).")

    try:
        full_history = get_resale_history_dual(asset_id, collectible_item_id)
        # truncate to avoid ResponseTooLarge
        if isinstance(full_history, dict) and isinstance(full_history.get("data"), list):
            data = full_history["data"]
            if len(data) > history_points:
                full_history = {**full_history, "data": data[-history_points:], "truncated": True}
        resale_history = full_history if isinstance(full_history, dict) else {"data": []}
    except HTTPException as e:
        market_notes.append(f"Resale-history unavailable (status {e.status_code}).")

    market = compute_market_stats(resale_data or {}, resale_history or {"data": []})

    # Trader scorecard (optional—if you already added it)
    trader = None
    try:
        trader = trader_scorecard(market, listings)  # if you have this function
    except Exception:
        trader = None

    return {
        "source": "roblox-only:full-analysis",
        "asset_id": asset_id,
        "collectible_item_id": collectible_item_id,
        "roblox": roblox,
        "market": market,
        "listings": listings,
        "trader": trader,
        "analysis": {
            "notes": market_notes,
            "generated_at": time.time(),
        },
    }

5) Add the background “hot refresher” loop

Paste this:

async def hot_refresher_loop():
    while True:
        try:
            # pick the hottest items (by request count)
            keys = list(HOT_COUNT.keys())
            keys.sort(key=lambda k: HOT_COUNT[k], reverse=True)

            refreshed = 0
            for key in keys:
                if refreshed >= MAX_REFRESH_PER_CYCLE:
                    break

                # only refresh if seen recently (last 30 minutes)
                last_seen = HOT_LAST_SEEN.get(key, 0.0)
                if (time.time() - last_seen) > 1800:
                    continue

                row = cache_get(key)
                if row and (time.time() - row["updated_at"]) < HOT_REFRESH_SECONDS:
                    continue  # already fresh enough

                # parse asset id
                if not key.startswith("item:"):
                    continue
                asset_id = int(key.split(":", 1)[1])

                # throttle upstream so we don't get rate-limited
                await _global_throttle()

                # refresh and store
                payload = build_compact_item_payload(asset_id, history_points=120)
                cache_put(key, payload)
                refreshed += 1

            await asyncio.sleep(SLEEP_TICK_SECONDS)
        except Exception:
            # never crash the loop
            await asyncio.sleep(SLEEP_TICK_SECONDS)

6) Start it on FastAPI startup

Paste this:

@app.on_event("startup")
async def on_startup():
    db_init()
    asyncio.create_task(hot_refresher_loop())

7) Update your /market/item/analyze endpoint to be cache-first

Replace your current analyze route with this one (same path, better behavior):

from fastapi import Body, Query

@app.post("/market/item/analyze")
def analyze_item_from_catalog_link(
    catalog_url: str = Body(..., embed=True),
    force_refresh: bool = Query(False),
):
    asset_id = _asset_id_from_catalog_url(catalog_url)
    if not asset_id:
        raise HTTPException(
            status_code=400,
            detail={"code": "INVALID_CATALOG_LINK", "message": "Catalog link must contain /catalog/<assetId>/."},
        )

    key = cache_key_for_asset(asset_id)
    mark_hot(key)

    row = cache_get(key)

    # Return cached immediately if fresh and not forcing refresh
    if row and is_fresh(row["updated_at"]) and not force_refresh:
        payload = row["payload"]
        payload["cache"] = {"hit": True, "fresh": True, "updated_at": row["updated_at"]}
        payload["catalog_url"] = catalog_url
        return payload

    # If we have stale cache, return it (stale-while-revalidate style)
    # and let background refresh catch up
    if row and not force_refresh:
        payload = row["payload"]
        payload["cache"] = {"hit": True, "fresh": False, "updated_at": row["updated_at"]}
        payload["catalog_url"] = catalog_url
        payload.setdefault("analysis", {}).setdefault("notes", []).append(
            "Returned cached data (stale). Background refresh scheduled."
        )
        return payload

    # Force refresh: compute now (may be slower)
    payload = build_compact_item_payload(asset_id, history_points=120)
    cache_put(key, payload)
    payload["cache"] = {"hit": False, "fresh": True, "updated_at": time.time()}
    payload["catalog_url"] = catalog_url
    return payload

What you get from this (the “Rolimon’s feel”)

First request might take a moment (fetch + compute)

After that:

answers are instant

data stays fresh-ish automatically

GPT never hits giant payload limits

and you stop seeing random “unavailable” from live endpoints as much because you’re not relying on a single moment-in-time request

IMPORTANT NOTE (so it doesn’t break)

This patch assumes you already have Roblox-only functions in your code:

get_catalog_details

_extract_collectible_item_id

get_resale_data_dual, get_resale_history_dual

get_resellers, summarize_resellers

compute_market_stats

(optional) trader_scorecard